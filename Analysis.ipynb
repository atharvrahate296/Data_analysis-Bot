{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autonomous data analysis Bot\n",
    "1. takes dataset files  as input(csv,xlsv,txt,json)\n",
    "2. performs operations like data cleaning,feature engineering,transformation,etc. autonoumously "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bz2\n",
    "import ast\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv as dtn\n",
    "from sklearn.impute import SimpleImputer\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### various functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(df):\n",
    "    print(\"Detecting anomalies...\\n\")\n",
    "    # Check for missing values\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any():\n",
    "            print(\"Found missing values! Imputing...\")\n",
    "            if df[col].dtype == 'object':\n",
    "                imputer = SimpleImputer(strategy='most_frequent')\n",
    "                df[col] = imputer.fit_transform(df[[col]]).ravel()\n",
    "            else:\n",
    "                imputer = SimpleImputer(strategy='mean')\n",
    "                df[col] = imputer.fit_transform(df[[col]]).ravel()\n",
    "\n",
    "    else:print(\"No missing values found !\")\n",
    "\n",
    "    # Check for duplicates\n",
    "    if len(df) != len(df.drop_duplicates()):\n",
    "        print(\"Found duplicate rows! Removing...\")\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    else:print(\"No duplicates found !\")\n",
    "\n",
    "    # Check for null values\n",
    "    if df.isnull().sum().any():\n",
    "        raise ValueError(\"Data contains null values after imputation!\")\n",
    "    else:print(\"No null values found !\")\n",
    "\n",
    "    # Check for invalid data in numeric columns\n",
    "    for col in df.select_dtypes(include=['int', 'float']):\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            raise ValueError(f\"Column {col} contains invalid data!\")\n",
    "    else:print(\"No invalid data found in numeric columns !\")\n",
    "    with open('Datasets/Processed/customer_shopping_data.csv','w')as f:\n",
    "        f.write(df.to_csv(index=False))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data):\n",
    "    dtn()\n",
    "    api_key = os.getenv(\"API_KEY_1\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"API key not found! Check your .env file.\")\n",
    "    genai.configure(api_key=api_key)\n",
    "    SYS = \"\"\"You are a Professional Data Analyst Chatbot. The user will provide you with a set of columns in a dataset. Your task is to identify and return ONLY the names of the columns that are most relevant and necessary for a comprehensive analysis. Return the column names as a Python list.\n",
    "    You dont need to return some common columns cosisting of ID,DOB or other irrelevant columns.\n",
    "    For example:\n",
    "    If the columns are ['customer_id', 'name', 'age', 'city', 'purchase_amount', 'date'], you should return:\n",
    "    ['age', 'city', 'purchase_amount', 'date']\n",
    "    \"\"\"\n",
    "    # Create the model with the updated system prompt\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        system_instruction=SYS\n",
    "    )\n",
    "    query = f\"The dataset contains the following columns: {data.columns.tolist()}. Please identify the columns that are necessary for the analysis task.\" \n",
    "    # Generate the response using the query\n",
    "    response = model.generate_content(query)\n",
    "    # return response.text.toList()\n",
    "    try:\n",
    "        col =  ast.literal_eval(response.text)\n",
    "        # print(col)\n",
    "        return col\n",
    "    except (SyntaxError, ValueError):\n",
    "        return None  # Or raise an exception, depending on your error handling preference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_data(data,columns):\n",
    "    # Drop unnecessary columns\n",
    "    selected_df = data[columns]\n",
    "    # Convert DataFrame to byte string using pickle\n",
    "    data_bytes = pickle.dumps(selected_df)\n",
    "    # Compress the byte string using bz2\n",
    "    compressed_data = bz2.compress(data_bytes, compresslevel=9)\n",
    "    return compressed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_insights(file_location,data,columns):\n",
    "    dtn()\n",
    "    # api_key = os.getenv(\"API_KEY_2\")\n",
    "    api_key = os.getenv(\"API_KEY_3\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"API key not found! Check your .env file.\")        \n",
    "    genai.configure(api_key=api_key)\n",
    "    SYST = '''You are a Professional Data Analyst Chatbot. You will be provided with a Pandas DataFrame named 'df' and a list of relevant columns identified in the previous step. Your task is to generate a concise data analysis report (maximum 3 paragraphs) summarizing key insights from the data, followed by Python code for data visualization that supports and illustrates these insights.  Assume the DataFrame 'df' is read directly from the file path specified in the `file_location` variable using pandas.\n",
    "\n",
    "    The report should:\n",
    "\n",
    "    *   Be written in a professional and clear tone.\n",
    "    *   Focus on the most important trends, patterns, and relationships within the data.\n",
    "    *   Include specific observations and quantifiable metrics (e.g., averages, distributions, correlations) to support your claims.\n",
    "    *   Present insights in bullet points for easy readability.\n",
    "\n",
    "    The Python code should:\n",
    "\n",
    "    *   Use the libraries Pandas, Matplotlib, and Seaborn.\n",
    "    *   Read the DataFrame 'df' directly from the path specified in the `file_location` variable using pandas.\n",
    "    *   Generate visualizations that reveal important trends, patterns, and relationships within the data.\n",
    "    *   Include descriptive statistics, distributions, count plots, scatter plots, box plots, correlation heatmaps, and time series analysis (if a date column is available).\n",
    "    *   Include appropriate titles, labels, and legends for clarity.\n",
    "    *   Be well-commented to explain the purpose of each step.\n",
    "    *   Be executable without errors, assuming the file is accessible at the path given by `file_location` and the relevant columns are present.\n",
    "    *   Focus on conciseness and clarity, providing a comprehensive overview of the data's key characteristics.\n",
    "\n",
    "    Example (for a customer shopping dataset):\n",
    "\n",
    "    **Input:**\n",
    "\n",
    "    *   Relevant Columns: `['age', 'city', 'purchase_amount', 'date']`\n",
    "    *   `file_location = 'Datasets/Raw/customer_shopping_data.csv'`\n",
    "\n",
    "    **Expected Output:**\n",
    "\n",
    "    **Data Analysis Report:**\n",
    "\n",
    "    The analysis of customer data reveals several key insights regarding purchasing behavior. Customers in the dataset range in age, with the largest group falling between 25 and 45 years old. Purchase amounts vary significantly, with a notable peak in transactions occurring during specific periods.\n",
    "\n",
    "    Key Insights Example:\n",
    "\n",
    "    *   The average purchase amount is $X, with a standard deviation of $Y.\n",
    "    *   Customers in City A tend to spend Z% more than customers in City B.\n",
    "    *   Purchase amounts show a positive correlation with age, particularly for customers over 50.\n",
    "    *   There is a significant increase in purchase activity during the months of November and December.\n",
    "\n",
    "    **Example Python Code:**\n",
    "\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    file_location = 'Datasets/Raw/customer_shopping_data.csv'\n",
    "    df = pd.read_csv(file_location)\n",
    "\n",
    "    # Set the style for seaborn plots\n",
    "    sns.set(style='whitegrid')\n",
    "\n",
    "    # 1. Age Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['age'], kde=True)\n",
    "    plt.title('Distribution of Customer Ages')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Purchase Amount by City\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.boxplot(x='city', y='purchase_amount', data=df)\n",
    "    plt.title('Purchase Amount Distribution by City')\n",
    "    plt.xlabel('City')\n",
    "    plt.ylabel('Purchase Amount')\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Purchase Amount over Time\n",
    "    df['date'] = pd.to_datetime(df['date')\n",
    "    df['month'] = df['date'].dt.month\n",
    "    monthly_purchases = df.groupby('month')['purchase_amount'].sum()\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    monthly_purchases.plot(kind='line', marker='o')\n",
    "    plt.title('Total Purchase Amount Over Time')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Total Purchase Amount')\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Correlation Heatmap\n",
    "    correlation_matrix = df[['age', 'purchase_amount']].corr()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "    plt.title('Correlation Heatmap of Age and Purchase Amount')\n",
    "    plt.show()'''\n",
    "\n",
    "    # Create the model with the updated system prompt\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        system_instruction=SYST\n",
    "    )\n",
    "    query = f\"df = {data}, columns = {columns}\"\n",
    "    # Generate the response using the query\n",
    "    response = model.generate_content(query)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    file_location = \"Datasets/Raw/customer_shopping_data.csv\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_location)\n",
    "    print(f\"Initial Data : \\n{df.head(5)}\\n\")\n",
    "    data = detect_anomalies(df)\n",
    "    print(f\"\\nCleaned Data : \\n {data.head(5)}\\n\")\n",
    "\n",
    "    columns = filter_data(data)\n",
    "    print(f\"Relevant Columns : {columns}\")\n",
    "\n",
    "    compressed_data = compress_data(data,columns)\n",
    "    # print(compressed_data)\n",
    "    result = gather_insights(file_location,compressed_data,columns)\n",
    "    # print(result)\n",
    "\n",
    "    filename = \"insights.py\"\n",
    "    # write the insights and code to a python file\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(result)\n",
    "    print(\"Insights and code saved to insights.py \\n Save or rename the file befire running the analysis task again!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data : \n",
      "  invoice_no customer_id  gender   age  category  quantity    price  \\\n",
      "0    I138884     C241288     NaN  28.0  Clothing         5  1500.40   \n",
      "1    I317333     C111565     NaN  21.0     Shoes         3  1800.51   \n",
      "2    I127801     C266599    Male   NaN  Clothing         1   300.08   \n",
      "3    I173702     C988172  Female  66.0     Shoes         5  3000.85   \n",
      "4    I337046     C189076  Female  53.0     Books         4    60.60   \n",
      "\n",
      "  payment_method invoice_date   shopping_mall  \n",
      "0    Credit Card     5/8/2022          Kanyon  \n",
      "1     Debit Card   12/12/2021  Forum Istanbul  \n",
      "2           Cash    9/11/2021       Metrocity  \n",
      "3    Credit Card   16/05/2021    Metropol AVM  \n",
      "4           Cash   24/10/2021          Kanyon  \n",
      "\n",
      "Detecting anomalies...\n",
      "\n",
      "Found missing values! Imputing...\n",
      "Found missing values! Imputing...\n",
      "No missing values found !\n",
      "No duplicates found !\n",
      "No null values found !\n",
      "No invalid data found in numeric columns !\n",
      "\n",
      "Cleaned Data : \n",
      "   invoice_no customer_id  gender        age  category  quantity    price  \\\n",
      "0    I138884     C241288  Female  28.000000  Clothing         5  1500.40   \n",
      "1    I317333     C111565  Female  21.000000     Shoes         3  1800.51   \n",
      "2    I127801     C266599    Male  43.427325  Clothing         1   300.08   \n",
      "3    I173702     C988172  Female  66.000000     Shoes         5  3000.85   \n",
      "4    I337046     C189076  Female  53.000000     Books         4    60.60   \n",
      "\n",
      "  payment_method invoice_date   shopping_mall  \n",
      "0    Credit Card     5/8/2022          Kanyon  \n",
      "1     Debit Card   12/12/2021  Forum Istanbul  \n",
      "2           Cash    9/11/2021       Metrocity  \n",
      "3    Credit Card   16/05/2021    Metropol AVM  \n",
      "4           Cash   24/10/2021          Kanyon  \n",
      "\n",
      "Relevant Columns : None\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\victus\\Documents\\Data_Science_Internship\\Project\\env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: None",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 12\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m columns \u001b[38;5;241m=\u001b[39m filter_data(data)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelevant Columns : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m compressed_data \u001b[38;5;241m=\u001b[39m \u001b[43mcompress_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# print(compressed_data)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m result \u001b[38;5;241m=\u001b[39m gather_insights(file_location,compressed_data,columns)\n",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m, in \u001b[0;36mcompress_data\u001b[1;34m(data, columns)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompress_data\u001b[39m(data,columns):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Drop unnecessary columns\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     selected_df \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Convert DataFrame to byte string using pickle\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     data_bytes \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(selected_df)\n",
      "File \u001b[1;32mc:\\Users\\victus\\Documents\\Data_Science_Internship\\Project\\env\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\victus\\Documents\\Data_Science_Internship\\Project\\env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
