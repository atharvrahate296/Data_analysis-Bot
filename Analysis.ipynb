{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autonomous data analysis Bot\n",
    "First Version of the entire Project...\n",
    "1. takes dataset files  as input(csv,xlsv,txt,json)\n",
    "2. performs operations like data cleaning,feature engineering,transformation,etc. autonoumously "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bz2\n",
    "import ast\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv as dtn\n",
    "from sklearn.impute import SimpleImputer\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### various functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(df):\n",
    "    print(\"Detecting anomalies...\\n\")\n",
    "    # Check for missing values\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any():\n",
    "            print(\"Found missing values! Imputing...\")\n",
    "            if df[col].dtype == 'object':\n",
    "                imputer = SimpleImputer(strategy='most_frequent')\n",
    "                df[col] = imputer.fit_transform(df[[col]]).ravel()\n",
    "            else:\n",
    "                imputer = SimpleImputer(strategy='mean')\n",
    "                df[col] = imputer.fit_transform(df[[col]]).ravel()\n",
    "\n",
    "    else:print(\"No missing values found !\")\n",
    "\n",
    "    # Check for duplicates\n",
    "    if len(df) != len(df.drop_duplicates()):\n",
    "        print(\"Found duplicate rows! Removing...\")\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    else:print(\"No duplicates found !\")\n",
    "\n",
    "    # Check for null values\n",
    "    if df.isnull().sum().any():\n",
    "        raise ValueError(\"Data contains null values after imputation!\")\n",
    "    else:print(\"No null values found !\")\n",
    "\n",
    "    # Check for invalid data in numeric columns\n",
    "    for col in df.select_dtypes(include=['int', 'float']):\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            raise ValueError(f\"Column {col} contains invalid data!\")\n",
    "    else:print(\"No invalid data found in numeric columns !\")\n",
    "    with open('Datasets/Processed/customer_shopping_data.csv','w')as f:\n",
    "        f.write(df.to_csv(index=False))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data):\n",
    "    dtn()\n",
    "    api_key = os.getenv(\"API_KEY_1\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"API key not found! Check your .env file.\")\n",
    "    genai.configure(api_key=api_key)\n",
    "    SYS = \"\"\"You are a Professional Data Analyst Chatbot. The user will provide you with a set of columns in a dataset. Your task is to identify and return ONLY the names of the columns that are most relevant and necessary for a comprehensive analysis. Return the column names as a Python list.\n",
    "    You dont need to return some common columns cosisting of ID,DOB or other irrelevant columns.\n",
    "    For example:\n",
    "    If the columns are ['customer_id', 'name', 'age', 'city', 'purchase_amount', 'date'], you should return:\n",
    "    ['age', 'city', 'purchase_amount', 'date']\n",
    "    \"\"\"\n",
    "    # Create the model with the updated system prompt\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        system_instruction=SYS\n",
    "    )\n",
    "    query = f\"The dataset contains the following columns: {data.columns.tolist()}. Please identify the columns that are necessary for the analysis task.\" \n",
    "    # Generate the response using the query\n",
    "    response = model.generate_content(query)\n",
    "    # return response.text.toList()\n",
    "    try:\n",
    "        col =  ast.literal_eval(response.text)\n",
    "        # print(col)\n",
    "        return col\n",
    "    except (SyntaxError, ValueError):\n",
    "        return None  # Or raise an exception, depending on your error handling preference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_data(data,columns):\n",
    "    # Drop unnecessary columns\n",
    "    selected_df = data[columns]\n",
    "    # Convert DataFrame to byte string using pickle\n",
    "    data_bytes = pickle.dumps(selected_df)\n",
    "    # Compress the byte string using bz2\n",
    "    compressed_data = bz2.compress(data_bytes, compresslevel=9)\n",
    "    return compressed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_insights(file_location,data,columns):\n",
    "    dtn()\n",
    "    # api_key = os.getenv(\"API_KEY_2\")\n",
    "    api_key = os.getenv(\"API_KEY_3\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"API key not found! Check your .env file.\")        \n",
    "    genai.configure(api_key=api_key)\n",
    "    SYST = '''You are a Professional Data Analyst Chatbot. You will be provided with a Pandas DataFrame named 'df' and a list of relevant columns identified in the previous step. Your task is to generate a concise data analysis report (maximum 3 paragraphs) summarizing key insights from the data, followed by Python code for data visualization that supports and illustrates these insights.  Assume the DataFrame 'df' is read directly from the file path specified in the `file_location` variable using pandas.\n",
    "\n",
    "    The report should:\n",
    "\n",
    "    *   Be written in a professional and clear tone.\n",
    "    *   Focus on the most important trends, patterns, and relationships within the data.\n",
    "    *   Include specific observations and quantifiable metrics (e.g., averages, distributions, correlations) to support your claims.\n",
    "    *   Present insights in bullet points for easy readability.\n",
    "\n",
    "    The Python code should:\n",
    "\n",
    "    *   Use the libraries Pandas, Matplotlib, and Seaborn.\n",
    "    *   Read the DataFrame 'df' directly from the path specified in the `file_location` variable using pandas.\n",
    "    *   Generate visualizations that reveal important trends, patterns, and relationships within the data.\n",
    "    *   Include descriptive statistics, distributions, count plots, scatter plots, box plots, correlation heatmaps, and time series analysis (if a date column is available).\n",
    "    *   Include appropriate titles, labels, and legends for clarity.\n",
    "    *   Be well-commented to explain the purpose of each step.\n",
    "    *   Be executable without errors, assuming the file is accessible at the path given by `file_location` and the relevant columns are present.\n",
    "    *   Focus on conciseness and clarity, providing a comprehensive overview of the data's key characteristics.\n",
    "\n",
    "    Example (for a customer shopping dataset):\n",
    "\n",
    "    **Input:**\n",
    "\n",
    "    *   Relevant Columns: `['age', 'city', 'purchase_amount', 'date']`\n",
    "    *   `file_location = 'Datasets/Raw/customer_shopping_data.csv'`\n",
    "\n",
    "    **Expected Output:**\n",
    "\n",
    "    **Data Analysis Report:**\n",
    "\n",
    "    The analysis of customer data reveals several key insights regarding purchasing behavior. Customers in the dataset range in age, with the largest group falling between 25 and 45 years old. Purchase amounts vary significantly, with a notable peak in transactions occurring during specific periods.\n",
    "\n",
    "    Key Insights Example:\n",
    "\n",
    "    *   The average purchase amount is $X, with a standard deviation of $Y.\n",
    "    *   Customers in City A tend to spend Z% more than customers in City B.\n",
    "    *   Purchase amounts show a positive correlation with age, particularly for customers over 50.\n",
    "    *   There is a significant increase in purchase activity during the months of November and December.\n",
    "\n",
    "    **Example Python Code:**\n",
    "\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    file_location = 'Datasets/Raw/customer_shopping_data.csv'\n",
    "    df = pd.read_csv(file_location)\n",
    "\n",
    "    # Set the style for seaborn plots\n",
    "    sns.set(style='whitegrid')\n",
    "\n",
    "    # 1. Age Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['age'], kde=True)\n",
    "    plt.title('Distribution of Customer Ages')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Purchase Amount by City\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.boxplot(x='city', y='purchase_amount', data=df)\n",
    "    plt.title('Purchase Amount Distribution by City')\n",
    "    plt.xlabel('City')\n",
    "    plt.ylabel('Purchase Amount')\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Purchase Amount over Time\n",
    "    df['date'] = pd.to_datetime(df['date')\n",
    "    df['month'] = df['date'].dt.month\n",
    "    monthly_purchases = df.groupby('month')['purchase_amount'].sum()\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    monthly_purchases.plot(kind='line', marker='o')\n",
    "    plt.title('Total Purchase Amount Over Time')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Total Purchase Amount')\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Correlation Heatmap\n",
    "    correlation_matrix = df[['age', 'purchase_amount']].corr()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "    plt.title('Correlation Heatmap of Age and Purchase Amount')\n",
    "    plt.show()'''\n",
    "\n",
    "    # Create the model with the updated system prompt\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        system_instruction=SYST\n",
    "    )\n",
    "    query = f\"df = {data}, columns = {columns}\"\n",
    "    # Generate the response using the query\n",
    "    response = model.generate_content(query)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    file_location = \"Datasets/Raw/customer_shopping_data.csv\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_location)\n",
    "    print(f\"Initial Data : \\n{df.head(5)}\\n\")\n",
    "    data = detect_anomalies(df)\n",
    "    print(f\"\\nCleaned Data : \\n {data.head(5)}\\n\")\n",
    "\n",
    "    columns = filter_data(data)\n",
    "    print(f\"Relevant Columns : {columns}\")\n",
    "\n",
    "    compressed_data = compress_data(data,columns)\n",
    "    # print(compressed_data)\n",
    "    result = gather_insights(file_location,compressed_data,columns)\n",
    "    # print(result)\n",
    "\n",
    "    filename = \"insights.py\"\n",
    "    # write the insights and code to a python file\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(result)\n",
    "    print(\"Insights and code saved to insights.py \\n Save or rename the file befire running the analysis task again!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
